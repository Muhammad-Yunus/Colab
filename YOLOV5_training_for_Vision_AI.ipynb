{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOV5-training-for-Vision-AI.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Colab/blob/main/YOLOV5_training_for_Vision_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will guide you to train your own AI model using YOLOv5!"
      ],
      "metadata": {
        "id": "4wCnaloE6aew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Choose **GPU** in **Runtime** if not already selected by navigating to `Runtime --> Change Runtime Type --> Hardware accelerator --> GPU`"
      ],
      "metadata": {
        "id": "6VrcRq-U6G2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Clone repo, install dependencies and check PyTorch and GPU"
      ],
      "metadata": {
        "id": "K8prsdf8u8Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install python3.7\n",
        "\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "\n",
        "!sudo update-alternatives --config python3\n",
        "\n",
        "!sudo apt install python3-pip\n",
        "\n",
        "!sudo apt-get install python3.7-distutils"
      ],
      "metadata": {
        "id": "KHfw8xVq6eG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Seeed-Studio/yolov5-swift  # clone\n",
        "%cd yolov5-swift\n"
      ],
      "metadata": {
        "id": "ZfnLgOjgu8ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a706faa9-75a7-45a5-beee-bcd5a53ee398"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5-swift'...\n",
            "remote: Enumerating objects: 9424, done.\u001b[K\n",
            "remote: Total 9424 (delta 0), reused 0 (delta 0), pack-reused 9424 (from 1)\u001b[K\n",
            "Receiving objects: 100% (9424/9424), 22.39 MiB | 12.54 MiB/s, done.\n",
            "Resolving deltas: 100% (6311/6311), done.\n",
            "/content/yolov5-swift/yolov5-swift\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from google.colab import files\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "metadata": {
        "id": "TQBOodNPv_59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e36f2a1-94da-4089-b0cc-5c3723b648be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 2.4.0+cu121 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15102MB, multi_processor_count=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.20.1"
      ],
      "metadata": {
        "id": "jfpMyYVq3ENd",
        "outputId": "1ac8c15d-19d1-46cf-fd16-6dd7ae923012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \u001b[33m\u001b[1mwarning  libmamba\u001b[m Added empty dependency for problem type SOLVER_RULE_UPDATE\n",
            "\b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bfailed\n",
            "\n",
            "LibMambaUnsatisfiableError: Encountered problems while solving:\n",
            "  - package numpy-1.20.1-py37h620df1f_0 requires python >=3.7,<3.8.0a0, but none of the providers can be installed\n",
            "\n",
            "Could not solve for environment specs\n",
            "The following packages are incompatible\n",
            "├─ \u001b[32mnumpy 1.20.1 \u001b[0m is installable with the potential options\n",
            "│  ├─ \u001b[32mnumpy 1.20.1\u001b[0m would require\n",
            "│  │  └─ \u001b[32mpython >=3.7,<3.8.0a0 \u001b[0m, which can be installed;\n",
            "│  ├─ \u001b[32mnumpy 1.20.1\u001b[0m would require\n",
            "│  │  └─ \u001b[32mpython >=3.8,<3.9.0a0 \u001b[0m, which can be installed;\n",
            "│  └─ \u001b[32mnumpy 1.20.1\u001b[0m would require\n",
            "│     └─ \u001b[32mpython >=3.9,<3.10.0a0 \u001b[0m, which can be installed;\n",
            "└─ \u001b[31mpin-2\u001b[0m is not installable because it requires\n",
            "   └─ \u001b[31mpython 3.10** \u001b[0m, which conflicts with any installable versions previously reported.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "numpy.version.version"
      ],
      "metadata": {
        "id": "BRstnLkd5-OQ",
        "outputId": "9904535b-110c-42e5-c086-1ec1a4400cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.26.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.** Set up environment"
      ],
      "metadata": {
        "id": "xPagIdHSaY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ],
      "metadata": {
        "id": "C32Eow5zafe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4.** Copy and paste the displayed code snippet from Roboflow on to the code cell below"
      ],
      "metadata": {
        "id": "fbUvo-BXefbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=center><img width=500 src=\"https://files.seeedstudio.com/wiki/YOLOV5/81.png\"/></div>"
      ],
      "metadata": {
        "id": "fmJjFIEpfF44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"uwcaZm715IIKM3Ej7ix5\")\n",
        "print(rf.workspace())\n",
        "project = rf.workspace(\"learning-k0ow4\").project(\"lego-classification-brvrs\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")\n"
      ],
      "metadata": {
        "id": "2su7XslvY4S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "%cat {dataset.location}/data.yaml"
      ],
      "metadata": {
        "id": "Fg4hIoyzE-Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5.** Download a pre-trained model suitable for our training"
      ],
      "metadata": {
        "id": "bX1U12sNFRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt"
      ],
      "metadata": {
        "id": "Db3JVwsPFp5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6.** Start training"
      ],
      "metadata": {
        "id": "YDBfSV8exwbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights\n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ],
      "metadata": {
        "id": "I6_oEXQqG8B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --img 192 --batch 64 --epochs 100 --data {dataset.location}/data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache"
      ],
      "metadata": {
        "id": "aCzjlGRSzs_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7.** Export TensorFlow Lite file"
      ],
      "metadata": {
        "id": "gpmLzJJ6M6_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite"
      ],
      "metadata": {
        "id": "zY8GOqDKM41s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8.** Convert TensorFlow Lite to UF2 file"
      ],
      "metadata": {
        "id": "zfbD2N2eMkV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UF2 is a file format, developed by Microsoft. Seeed uses this format to convert .tflite to .uf2, allowing tflite files to be stored on the AIoT devices launched by Seeed. Currently Seeed's devices support up to 4 models, each model (.tflite) is less than 1M .\n",
        "\n",
        "You can specify the model to be placed in the corresponding index with -t.\n",
        "\n",
        "For example:\n",
        "\n",
        "- `-t 1`: index 1\n",
        "- `-t 2`: index 2"
      ],
      "metadata": {
        "id": "rtpBb33HkSC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place the model to index 1\n",
        "!python3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2\n",
        "%cp model-1.uf2 ../"
      ],
      "metadata": {
        "id": "2f3-VHKGNRTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9.** Download the trained model file"
      ],
      "metadata": {
        "id": "S8MXi72tNKJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"/content/model-1.uf2\")"
      ],
      "metadata": {
        "id": "61orx1ttMJoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the file that we will load into the SenseCAP A1101/ Grove - Vision AI Module to perform the inference!"
      ],
      "metadata": {
        "id": "CHDAOmaYNfc1"
      }
    }
  ]
}