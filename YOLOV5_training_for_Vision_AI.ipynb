{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOV5-training-for-Vision-AI.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Yunus/Colab/blob/main/YOLOV5_training_for_Vision_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will guide you to train your own AI model using YOLOv5!"
      ],
      "metadata": {
        "id": "4wCnaloE6aew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.** Choose **GPU** in **Runtime** if not already selected by navigating to `Runtime --> Change Runtime Type --> Hardware accelerator --> GPU`"
      ],
      "metadata": {
        "id": "6VrcRq-U6G2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.** Clone repo, install dependencies and check PyTorch and GPU"
      ],
      "metadata": {
        "id": "K8prsdf8u8Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Seeed-Studio/yolov5-swift  # clone\n",
        "%cd yolov5-swift\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from google.colab import files\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "metadata": {
        "id": "ZfnLgOjgu8ng",
        "outputId": "bc4e3cf8-332e-4636-f37b-1a359b1665ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5-swift'...\n",
            "remote: Enumerating objects: 9424, done.\u001b[K\n",
            "remote: Total 9424 (delta 0), reused 0 (delta 0), pack-reused 9424 (from 1)\u001b[K\n",
            "Receiving objects: 100% (9424/9424), 22.37 MiB | 27.15 MiB/s, done.\n",
            "Resolving deltas: 100% (6303/6303), done.\n",
            "/content/yolov5-swift/yolov5-swift\n",
            "Collecting numpy==1.20\n",
            "  Downloading numpy-1.20.0.zip (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)\u001b[0m\u001b[31m\n",
            "\u001b[0mSetup complete. Using torch 2.4.0+cu121 _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15102MB, multi_processor_count=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy==1.18.5"
      ],
      "metadata": {
        "id": "TQBOodNPv_59",
        "outputId": "703c7752-edff-48a1-942f-2d1d27d931f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5.zip (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3.** Set up environment"
      ],
      "metadata": {
        "id": "xPagIdHSaY6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ],
      "metadata": {
        "id": "C32Eow5zafe_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4.** Copy and paste the displayed code snippet from Roboflow on to the code cell below"
      ],
      "metadata": {
        "id": "fbUvo-BXefbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=center><img width=500 src=\"https://files.seeedstudio.com/wiki/YOLOV5/81.png\"/></div>"
      ],
      "metadata": {
        "id": "fmJjFIEpfF44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"uwcaZm715IIKM3Ej7ix5\")\n",
        "print(rf.workspace())\n",
        "project = rf.workspace(\"learning-k0ow4\").project(\"lego-classification-brvrs\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov5\")\n",
        ""
      ],
      "metadata": {
        "id": "2su7XslvY4S1",
        "outputId": "954f9331-80b1-45a1-9b72-1e1cc7aca9c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.43)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.7.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.53.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "{\n",
            "  \"name\": \"Learning\",\n",
            "  \"url\": \"learning-k0ow4\",\n",
            "  \"projects\": [\n",
            "    \"learning-k0ow4/hard-hat-sample-xdvg2\",\n",
            "    \"learning-k0ow4/lego-classification-brvrs\"\n",
            "  ]\n",
            "}\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in /content/datasets/Lego-Classification-1 to yolov5pytorch:: 100%|██████████| 322/322 [00:00<00:00, 2433.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to /content/datasets/Lego-Classification-1 in yolov5pytorch:: 100%|██████████| 86/86 [00:00<00:00, 8604.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "%cat {dataset.location}/data.yaml"
      ],
      "metadata": {
        "id": "Fg4hIoyzE-Qe",
        "outputId": "4975fd77-32ec-48f6-a34d-7daece2b138b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:\n",
            "- blue_plane\n",
            "- gray_plane\n",
            "- red_plane\n",
            "nc: 3\n",
            "roboflow:\n",
            "  license: CC BY 4.0\n",
            "  project: lego-classification-brvrs\n",
            "  url: https://universe.roboflow.com/muhammad-yunus-27fje/lego-classification-brvrs/dataset/1\n",
            "  version: 1\n",
            "  workspace: muhammad-yunus-27fje\n",
            "test: ../test/images\n",
            "train: /content/datasets/Lego-Classification-1/train/images\n",
            "val: /content/datasets/Lego-Classification-1/valid/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5.** Download a pre-trained model suitable for our training"
      ],
      "metadata": {
        "id": "bX1U12sNFRM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt"
      ],
      "metadata": {
        "id": "Db3JVwsPFp5R",
        "outputId": "954fa4cb-1f83-49fc-ad79-7c91f40ce378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-28 01:59:00--  https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/483523906/2e3a41ce-abc0-446a-9573-1173f4e58024?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240828%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240828T015900Z&X-Amz-Expires=300&X-Amz-Signature=ec3c222d147d784e73b89888f42c1d1ec68dcfd8e45ad916fc56c0ab960e5f57&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=483523906&response-content-disposition=attachment%3B%20filename%3Dyolov5n6-xiao.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-08-28 01:59:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/483523906/2e3a41ce-abc0-446a-9573-1173f4e58024?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240828%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240828T015900Z&X-Amz-Expires=300&X-Amz-Signature=ec3c222d147d784e73b89888f42c1d1ec68dcfd8e45ad916fc56c0ab960e5f57&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=483523906&response-content-disposition=attachment%3B%20filename%3Dyolov5n6-xiao.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1293269 (1.2M) [application/octet-stream]\n",
            "Saving to: ‘yolov5n6-xiao.pt’\n",
            "\n",
            "yolov5n6-xiao.pt    100%[===================>]   1.23M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-08-28 01:59:01 (17.6 MB/s) - ‘yolov5n6-xiao.pt’ saved [1293269/1293269]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6.** Start training"
      ],
      "metadata": {
        "id": "YDBfSV8exwbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights\n",
        "- **name:** result names\n",
        "- **nosave:** only save the final checkpoint\n",
        "- **cache:** cache images for faster training"
      ],
      "metadata": {
        "id": "I6_oEXQqG8B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --img 192 --batch 64 --epochs 100 --data {dataset.location}/data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache"
      ],
      "metadata": {
        "id": "aCzjlGRSzs_F",
        "outputId": "71451605-6e10-4d63-cda2-7799bcbb476a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "2024-08-28 01:59:11.881578: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n",
            "2024-08-28 01:59:11.881612: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n6-xiao.pt, cfg=yolov5n6-xiao.yaml, data=/content/datasets/Lego-Classification-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=64, imgsz=192, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov5n6_results, exist_ok=False, quad=False, two_linear_lr=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/Seeed-Studio/yolov5-swif ✅\n",
            "YOLOv5 🚀 v0.1.0-alpha-20-gc0abd26 torch 2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "/content/yolov5-swift/train.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(weights, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       880  models.common.Conv                      [3, 8, 6, 2, 2]               \n",
            "  1                -1  1      1184  models.common.Conv                      [8, 16, 3, 2]                 \n",
            "  2                -1  1      1248  models.common.C3                        [16, 16, 1]                   \n",
            "  3                -1  1      3504  models.common.Conv                      [16, 24, 3, 2]                \n",
            "  4                -1  2      4224  models.common.C3                        [24, 24, 2]                   \n",
            "  5                -1  1     10464  models.common.Conv                      [24, 48, 3, 2]                \n",
            "  6                -1  3     22368  models.common.C3                        [48, 48, 3]                   \n",
            "  7                -1  1     31248  models.common.Conv                      [48, 72, 3, 2]                \n",
            "  8                -1  1     23760  models.common.C3                        [72, 72, 1]                   \n",
            "  9                -1  1     62400  models.common.Conv                      [72, 96, 3, 2]                \n",
            " 10                -1  1     42048  models.common.C3                        [96, 96, 1]                   \n",
            " 11                -1  1     23328  models.common.SPPF                      [96, 96, 5]                   \n",
            " 12                -1  1      7056  models.common.Conv                      [96, 72, 1, 1]                \n",
            " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 15                -1  1     28944  models.common.C3                        [144, 72, 1, False]           \n",
            " 16                -1  1      3552  models.common.Conv                      [72, 48, 1, 1]                \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1     12960  models.common.C3                        [96, 48, 1, False]            \n",
            " 20                -1  1      1200  models.common.Conv                      [48, 24, 1, 1]                \n",
            " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1      3312  models.common.C3                        [48, 24, 1, False]            \n",
            " 24                -1  1      5232  models.common.Conv                      [24, 24, 3, 2]                \n",
            " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1     10656  models.common.C3                        [48, 48, 1, False]            \n",
            " 27                -1  1     20832  models.common.Conv                      [48, 48, 3, 2]                \n",
            " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
            " 29                -1  1     25488  models.common.C3                        [96, 72, 1, False]            \n",
            " 30                -1  1     46800  models.common.Conv                      [72, 72, 3, 2]                \n",
            " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
            " 32                -1  1     46656  models.common.C3                        [144, 96, 1, False]           \n",
            " 33  [23, 26, 29, 32]  1      5856  models.yolo.Detect                      [3, [[9, 12, 21, 18, 17, 41], [40, 32, 38, 73, 86, 57], [69, 120, 144, 123, 109, 240], [331, 182, 216, 326, 467, 375]], [24, 48, 72, 96]]\n",
            "Model Summary: 356 layers, 445200 parameters, 445200 gradients, 0.8 GFLOPs\n",
            "\n",
            "Transferred 450/459 items from yolov5n6-xiao.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 75 weight (no decay), 79 weight, 79 bias\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py:161: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/Lego-Classification-1/train/labels' images and labels...33 found, 0 missing, 0 empty, 0 corrupt: 100% 33/33 [00:00<00:00, 587.44it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/Lego-Classification-1/train/labels.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yolov5-swift/train.py\", line 646, in <module>\n",
            "    main(opt)\n",
            "  File \"/content/yolov5-swift/train.py\", line 542, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"/content/yolov5-swift/train.py\", line 225, in train\n",
            "    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
            "  File \"/content/yolov5-swift/utils/datasets.py\", line 100, in create_dataloader\n",
            "    dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
            "  File \"/content/yolov5-swift/utils/datasets.py\", line 443, in __init__\n",
            "    bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/__init__.py\", line 324, in __getattr__\n",
            "    raise AttributeError(__former_attrs__[attr])\n",
            "AttributeError: module 'numpy' has no attribute 'int'.\n",
            "`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
            "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'inf'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7.** Export TensorFlow Lite file"
      ],
      "metadata": {
        "id": "gpmLzJJ6M6_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite"
      ],
      "metadata": {
        "id": "zY8GOqDKM41s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8.** Convert TensorFlow Lite to UF2 file"
      ],
      "metadata": {
        "id": "zfbD2N2eMkV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UF2 is a file format, developed by Microsoft. Seeed uses this format to convert .tflite to .uf2, allowing tflite files to be stored on the AIoT devices launched by Seeed. Currently Seeed's devices support up to 4 models, each model (.tflite) is less than 1M .\n",
        "\n",
        "You can specify the model to be placed in the corresponding index with -t.\n",
        "\n",
        "For example:\n",
        "\n",
        "- `-t 1`: index 1\n",
        "- `-t 2`: index 2"
      ],
      "metadata": {
        "id": "rtpBb33HkSC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Place the model to index 1\n",
        "!python3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2\n",
        "%cp model-1.uf2 ../"
      ],
      "metadata": {
        "id": "2f3-VHKGNRTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9.** Download the trained model file"
      ],
      "metadata": {
        "id": "S8MXi72tNKJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"/content/model-1.uf2\")"
      ],
      "metadata": {
        "id": "61orx1ttMJoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is the file that we will load into the SenseCAP A1101/ Grove - Vision AI Module to perform the inference!"
      ],
      "metadata": {
        "id": "CHDAOmaYNfc1"
      }
    }
  ]
}